# 07 Â· Calidad, testing y CI

> La calidad no aparece al final.  
> Se construye todos los dÃ­as, en pequeÃ±o.

Este capÃ­tulo describe cÃ³mo aseguramos
**calidad mÃ­nima continua**
en proyectos de Data Science,
sin frenar el trabajo ni imponer prÃ¡cticas irreales.

No buscamos cobertura perfecta.
Buscamos **confianza razonable**.

---

## QuÃ© entendemos por calidad en Data Science

En Data Science, calidad no es solo accuracy.

TambiÃ©n es:

- estabilidad
- reproducibilidad
- capacidad de detectar errores temprano
- confianza para cambiar cosas sin romper todo

Los tests y la CI no estÃ¡n para controlar,
sino para **reducir riesgos silenciosos**.

---

## ğŸ”’ MUST Â· Testing mÃ­nimo esperado

No todo es testeable en Data Science,
y fingir que lo es solo genera frustraciÃ³n.

Pero hay cosas que **sÃ­ o sÃ­** deben validarse.

---

### QuÃ© sÃ­ testear

- funciones crÃ­ticas
- contratos de entrada y salida
- tipos y shapes
- invariantes importantes del dominio
- casos borde conocidos

Estos tests suelen ser:

- rÃ¡pidos
- deterministas
- baratos de mantener

---

### QuÃ© no es prioridad (al inicio)

- mÃ©tricas del modelo
- resultados exactos de entrenamiento
- notebooks exploratorios
- benchmarks complejos

Esto no significa â€œnuncaâ€,
significa **no bloquear el avance temprano**.

---

??? example "Ejemplo Â· Test pequeÃ±o pero valioso"
    ```python
    def test_preprocess_output_shape():
        X = preprocess(raw_data)
        assert X.shape[1] == EXPECTED_NUM_FEATURES
    ```

    Este test:
    - no depende del modelo
    - detecta errores comunes
    - falla rÃ¡pido y de forma clara

---

## ğŸŒ± DESEABLE Â· Tests mÃ¡s expresivos

A medida que el proyecto madura,
algunos tests adicionales agregan mucho valor.

---

### Tests de invariantes del dominio

Reglas del negocio suelen ser mÃ¡s estables
que los modelos.

---

??? example "Ejemplo Â· Test de invariante"
    ```python
    def test_predictions_are_non_negative():
        preds = model.predict(sample_input)
        assert (preds >= 0).all()
    ```

    No valida performance.
    Valida una **regla del dominio**.

---

### Tests de errores esperados

Asegurarse de que el cÃ³digo falle
de forma controlada tambiÃ©n es calidad.

---

??? example "Ejemplo Â· Error esperado"
    ```python
    def test_empty_input_raises_error():
        with pytest.raises(ValueError):
            preprocess([])
    ```

---

## Notebooks y testing

Los notebooks **no se testean directamente**.

La lÃ³gica que merece tests:

- se mueve a `src/`
- se importa desde notebooks
- se valida en tests aislados

Esto mantiene:

- notebooks livianos
- tests simples
- menos duplicaciÃ³n

---

## ğŸ”’ MUST Â· CI como acuerdo automÃ¡tico

Usamos **GitHub Actions** como sistema de CI.

El objetivo no es sofisticaciÃ³n,
es **automatizar acuerdos bÃ¡sicos del equipo**.

La CI corre:

- en cada Pull Request
- en cada merge a `main`

??? info "CÃ³mo implementar CI con GitHub Actions usando `uv` (guÃ­a rÃ¡pida)"
    En este playbook usamos **`uv`** como estÃ¡ndar para
    entornos y dependencias.
    La CI sigue exactamente el mismo flujo que local.

    ### DocumentaciÃ³n recomendada
    - [GuÃ­a oficial Â· Build and test Python (GitHub Actions)](https://docs.github.com/actions/guides/building-and-testing-python)
    - [DocumentaciÃ³n oficial de `uv`](https://docs.astral.sh/uv/)

    ### Idea general
    1. El repo define dependencias en `pyproject.toml`
       y versiones exactas en `uv.lock`
    2. La CI recrea el entorno con `uv sync`
    3. Los tests se ejecutan dentro de ese entorno

    Esto garantiza que:
    - CI y local usan las mismas dependencias
    - los resultados son reproducibles
    - no hay installs â€œespecialesâ€ solo para CI

    ### Ejemplo mÃ­nimo de workflow (referencia)

    ```yaml
    name: CI

    on:
      pull_request:
      push:
        branches: [ "main" ]

    jobs:
      test:
        runs-on: ubuntu-latest

        steps:
          - uses: actions/checkout@v4

          - name: Set up Python
            uses: actions/setup-python@v5
            with:
              python-version: "3.11"

          - name: Install uv
            run: |
              curl -Ls https://astral.sh/uv/install.sh | sh

          - name: Sync environment
            run: |
              uv sync

          - name: Run tests
            run: |
              uv run pytest
    ```

    Este workflow:
    - instala `uv`
    - recrea el entorno desde `uv.lock`
    - ejecuta los tests dentro del entorno del proyecto
    - falla antes de mergear si algo estÃ¡ roto

---

### QuÃ© deberÃ­a correr en CI (mÃ­nimo viable)

- instalaciÃ³n del entorno
- ejecuciÃ³n de tests rÃ¡pidos
- checks de calidad (lint / formato)

Si algo falla en CI,
el problema se detecta **antes de mergear**,
no despuÃ©s.

---

## CI vs testing local

Es importante entender la diferencia.

- **Testing local**  
  Feedback rÃ¡pido para quien desarrolla.
  Permite iterar sin fricciÃ³n.

- **CI**  
  GarantÃ­a compartida del equipo.
  Evita olvidos y errores humanos.

Uno no reemplaza al otro.
Se complementan.

---

## ğŸŒ± DESEABLE Â· Expandir CI con el tiempo

Cuando el proyecto crece,
la CI puede evolucionar para incluir:

- tests mÃ¡s costosos
- checks adicionales
- validaciones especÃ­ficas del proyecto

La clave es que:
> la CI crezca con el proyecto,
> no que lo bloquee desde el dÃ­a uno.

---

## Antipatrones (seÃ±ales de alerta)

- no hay tests â€œporque es researchâ€
- tests lentos que nadie corre
- CI que falla por razones irrelevantes
- ignorar CI para â€œavanzar mÃ¡s rÃ¡pidoâ€

Estos patrones suelen
pagar intereses altos despuÃ©s.

---

## RelaciÃ³n con el resto del playbook

- [`05 Â· EstÃ¡ndares`](05-estandares-de-codigo.md) define quÃ© esperamos del cÃ³digo
- [`06 Â· Git`](06-git-y-forma-de-trabajar.md) define cuÃ¡ndo se valida
- [`07 Â· CI`](07-calidad-testing-y-ci.md) lo hace automÃ¡tico
- [`09 Â· Lifecycle`](09-ml-lifecycle-y-produccion.md) lleva estos conceptos a producciÃ³n

La CI es el punto donde
las buenas intenciones se vuelven verificables.

---

## Cierre

Los tests no garantizan calidad.
Pero **la ausencia de tests garantiza incertidumbre**.

La CI no elimina errores.
Pero evita que lleguen tarde.

Ese es el objetivo.
