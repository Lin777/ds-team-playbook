# 08 Â· ExperimentaciÃ³n y trazabilidad

> Un experimento que no se puede explicar  
> es solo una anÃ©cdota.

Este capÃ­tulo define cÃ³mo experimentamos en Data Science
de forma **ordenada, trazable y razonablemente reproducible**.

No buscamos rigor acadÃ©mico perfecto.
Buscamos poder responder, con honestidad:
**quÃ© hicimos, por quÃ© lo hicimos y quÃ© aprendimos**.

---

## El problema que estamos tratando de evitar

Sin una forma clara de experimentar, aparecen patrones conocidos:

- notebooks con resultados â€œbuenosâ€ imposibles de repetir
- parÃ¡metros cambiados sin registro
- datasets que evolucionan sin dejar rastro
- decisiones tomadas por intuiciÃ³n sin evidencia clara

El resultado no es solo desorden,
es **pÃ©rdida de confianza en el trabajo hecho**.

---

## QuÃ© significa â€œreproducibleâ€ en la prÃ¡ctica

En Data Science, reproducible no siempre significa
obtener exactamente el mismo nÃºmero.

Significa:

- entender quÃ© variables importaron
- poder repetir el proceso con resultados comparables
- explicar por quÃ© algo funcionÃ³ (o no)

La reproducibilidad es un **continuo**, no un switch.

---

## Experimentar es un proceso, no un evento

Un buen experimento tiene, como mÃ­nimo:

- una hipÃ³tesis clara
- un cambio controlado
- una forma de evaluar resultados
- una conclusiÃ³n explÃ­cita

No todos los experimentos son exitosos.
Pero **todos deberÃ­an dejar aprendizaje**.

---

## ğŸ”’ MUST Â· Registrar los experimentos

Todo experimento relevante debe quedar registrado.

No importa la herramienta.
Importa que exista un rastro claro.

En este equipo usamos **Markdown** como formato base,
porque es:

- simple
- versionable
- fÃ¡cil de revisar
- independiente de herramientas

---

### Template oficial de experimentos

!!! tip
    Usamos un template estÃ¡ndar para reportar experimentos, lo puedes encontrar en [`../docs-templates/experiment-report-template.md`](../docs-templates/experiment-report-template.md).

Ese archivo define:

- quÃ© informaciÃ³n se espera
- cÃ³mo estructurar el reporte
- quÃ© preguntas responder

El capÃ­tulo explica *por quÃ©*.
El template muestra *cÃ³mo se ve en la prÃ¡ctica*.

---

??? info "CÃ³mo usar el template en la prÃ¡ctica"
    - Cada experimento relevante genera un archivo Markdown
    - El archivo vive en el repo (por ejemplo en `experiments/`)
    - No todos los experimentos tienen que ser â€œbuenosâ€
    - Todos deberÃ­an ser **entendibles**

---

## Notebooks como laboratorio

Los notebooks son excelentes para:

- explorar ideas
- analizar resultados
- comunicar hallazgos

Un notebook bien usado:

- narra el proceso
- muestra decisiones
- no es la Ãºnica fuente de verdad

Cuando un experimento importa:

- sus conclusiones se registran
- sus decisiones se documentan
- su resultado se resume fuera del notebook

---

??? example "Ejemplo Â· Buen experimento vs mal experimento"
    âŒ **Mal experimento**

    > â€œProbÃ© varias cosas y este modelo funcionÃ³ mejor.â€

    - sin hipÃ³tesis
    - sin contexto
    - imposible de reproducir

    ---

    âœ… **Buen experimento**

    > â€œReducir la dimensionalidad mejorÃ³ estabilidad,
    > pero no impacto significativamente la mÃ©trica principal.â€

    - hipÃ³tesis clara
    - cambio explÃ­cito
    - aprendizaje documentado

---

## Seeds, aleatoriedad y expectativas realistas

Fijar seeds ayuda a:

- debuggear
- comparar cambios
- reducir ruido innecesario

Pero obsesionarse con determinismo total
suele ser una falsa sensaciÃ³n de control.

Regla prÃ¡ctica:

- fija seeds cuando ayuden a entender
- no fuerces reproducibilidad artificial

---

## ğŸŒ± DESEABLE Â· Herramientas de tracking

Por defecto, el playbook es **tool-agnostic**.

Sin embargo, si el equipo o proyecto lo permite,
herramientas como **MLflow** pueden agregar valor:

- tracking automÃ¡tico de parÃ¡metros
- comparaciÃ³n de runs
- registro de artefactos

Esto es especialmente comÃºn en plataformas como Databricks.

Si no existe esa infraestructura,
Markdown sigue siendo una excelente opciÃ³n.

---

## ğŸŒ± DESEABLE Â· Datasets y versionado (nivel bÃ¡sico)

Idealmente, un experimento deberÃ­a dejar claro:

- quÃ© datos usÃ³
- de dÃ³nde salieron
- en quÃ© versiÃ³n estaban

No es necesario implementar sistemas complejos desde el inicio.

Un primer paso suficiente puede ser:

- hash del dataset
- fecha de extracciÃ³n
- referencia a la fuente

!!! tip
    Si quieres ir mÃ¡s allÃ¡,
    vale la pena explorar herramientas como DVC
    o enfoques de data versioning mÃ¡s formales.

---

## Antipatrones (seÃ±ales de alerta)

- â€œeste fue el mejor de muchosâ€
- resultados sin contexto
- experimentos sin registro
- notebooks como Ãºnica fuente de verdad

Estos patrones hacen difÃ­cil aprender,
incluso cuando el resultado es bueno.

---

## RelaciÃ³n con el resto del playbook

- [`03 Â· Entornos`](03-entornos-y-dependencias.md) â†’ reproducibilidad tÃ©cnica
- [`07 Â· CI`](07-calidad-testing-y-ci.md) â†’ reproducibilidad del cÃ³digo
- [`08 Â· Experimentos`](08-experimentacion-y-trazabilidad.md) â†’ reproducibilidad del proceso
- [`09 Â· Lifecycle`](09-ml-lifecycle-y-produccion.md) â†’ reproducibilidad en producciÃ³n

Este capÃ­tulo es el puente
entre explorar y decidir.

---

## Cierre

No todos los experimentos llevan a producciÃ³n.
Pero todos deberÃ­an dejar aprendizaje.

Registrar experimentos no es burocracia.
Es **respetar el trabajo ya hecho**.
